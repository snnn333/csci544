{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DkYIR26TT6e",
        "outputId": "92ec6283-1aa3-4244-869d-c6179d27be34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path = \"/content/drive/MyDrive/CSCI544/project/\""
      ],
      "metadata": {
        "id": "NfA4uRjGTVlF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest Tensorflow version.\n",
        "!pip install -q \"transformers==4.9.2\"\n",
        "!pip install -q \"datasets==1.11.0\""
      ],
      "metadata": {
        "id": "hvoblTYQe2oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FQGxBVV6TPAq"
      },
      "outputs": [],
      "source": [
        "from ast import literal_eval\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fatLhdpYeOA6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RvtVQmPqGGeY"
      },
      "outputs": [],
      "source": [
        "def bert_model(question, context):\n",
        "    #Model\n",
        "    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    #Tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    \n",
        "    encoding = tokenizer.encode_plus(text=question,text_pair=context)\n",
        "    inputs = encoding['input_ids']  #Token embeddings\n",
        "    sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
        "\n",
        "    output = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
        "    start_index = torch.argmax(output.start_logits)\n",
        "\n",
        "    end_index = torch.argmax(output.end_logits)\n",
        "\n",
        "    answer = ' '.join(tokens[start_index:end_index+1])\n",
        "\n",
        "    corrected_answer = ''\n",
        "\n",
        "    for word in answer.split():\n",
        "    \n",
        "        #If it's a subword token\n",
        "        if word[0:2] == '##':\n",
        "            corrected_answer += word[2:]\n",
        "        else:\n",
        "            corrected_answer += ' ' + word\n",
        "    \n",
        "\n",
        "    return corrected_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WSlD-3LuLLKo"
      },
      "outputs": [],
      "source": [
        "def extract_edu(cont_file):\n",
        "    # edu_out = open(path+\"output/education.txt\", 'w+')\n",
        "    edu_out = open(\"output/education.txt\", 'w+')\n",
        "    edu_info = {}\n",
        "    with open(cont_file, 'r') as f:\n",
        "        for line in f:\n",
        "            resu = list(literal_eval(line))\n",
        "            \n",
        "            id = resu[0]['id']\n",
        "            category = resu[0]['category'].lower()\n",
        "            \n",
        "            for sec in resu:\n",
        "\n",
        "                title = sec['title'].lower()\n",
        "\n",
        "                if \"education\" in title :\n",
        "\n",
        "                    ques_college = \"what is your college?\"\n",
        "                    ques_major = \"what is your major?\"\n",
        "                    ques_degree = \"what is your degree\"\n",
        "\n",
        "                    context = ' '.join(sec['content'])\n",
        "                    college = bert_model(ques_college, context)\n",
        "                    major = bert_model(ques_major, context)\n",
        "                    degree = bert_model(ques_degree, context)\n",
        "\n",
        "#                     print('{}, {}, {},{}, [{}]\\n'.format(id, college, major, degree, context))\n",
        "                    edu_out.write('{}, {}, {}, {}, [{}]\\n'.format(id, college, major, degree, context))\n",
        "                    edu_info[id] = '{}, {}, {}'.format(college, major, degree)\n",
        "            \n",
        "    return edu_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gDd1NWp6eFFG"
      },
      "outputs": [],
      "source": [
        "def extract_expr(expr_file):\n",
        "    # exp_out = open(path+\"output/exp_summary.txt\", 'w+')\n",
        "    exp_out = open(\"output/exp_summary.txt\", 'w+')\n",
        "    expr_info = {}\n",
        "    with open(expr_file, 'r') as f:\n",
        "\n",
        "        for line in f:\n",
        "            exps = json.loads(line)\n",
        "            id = exps['id']\n",
        "\n",
        "            wkex = \"I have \"+str(len(exps['experiences']))+ \" work experiences. \"\n",
        "            for exp in exps['experiences']:\n",
        "                jobtitle = exp['jobtitle']\n",
        "                startsdate = exp['startsdate']\n",
        "                endsdate = exp['endsdate']\n",
        "                if exp['jobduty']:\n",
        "                    jobduty = '. '.join(exp['jobduty'])\n",
        "\n",
        "                if endsdate:\n",
        "                    wkex += \"As a \"+jobtitle+\", I worked from \" + startsdate + \" to \" + endsdate + \". And my major duty is \"+jobduty\n",
        "                elif startsdate:\n",
        "                     wkex += \"As a \"+jobtitle+\", I worked from \" + startsdate + \" to now. And my major duty is \"+jobduty\n",
        "            exp_out.write(\"{}|{}\\n\".format(id, wkex))\n",
        "            expr_info[id] = wkex\n",
        "            \n",
        "    return expr_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4SQP-aImTPAu"
      },
      "outputs": [],
      "source": [
        "def extract_skill(skil_file):\n",
        "    skil_info = {}\n",
        "    with open(skil_file, 'r') as f:\n",
        "        for line in f:\n",
        "            id, skil = line.split(\":\")\n",
        "            skil_info[id] = list(literal_eval(skil))\n",
        "            \n",
        "    return skil_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iKG0P2LrTPAu"
      },
      "outputs": [],
      "source": [
        "def generate_intro(edu_info, expr_info, skil_info):\n",
        "    \n",
        "    all_intros = {}\n",
        "#     I got my ____(bachelor/master/PhD) degree in _____(major) from ______ (college).\n",
        "#     As a ___, I work from ___ (to ____), my major duty is __________.\n",
        "#     Iâ€™m proficient at _____(extract from skills).\n",
        "\n",
        "    for key in edu_info.keys():\n",
        "        intro = \"\"\n",
        "        college, major, degree = edu_info[key].split(',')\n",
        "        intro += \"I got my \"+ degree + \" in \"+ major + \" from \"+college + \". \"\n",
        "        intro += expr_info[key] + \" \"\n",
        "        intro += \"I'm proficient at \" + skil_info[key][0] + \", \" + skil_info[key][1] + \", and \" + skil_info[key][2]\n",
        "        \n",
        "        all_intros[key] = intro\n",
        "        \n",
        "    return all_intros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yf4WZSXSTPAv"
      },
      "outputs": [],
      "source": [
        "def save_intro(out_intro_file, intros):\n",
        "    with open(out_intro_file, \"w\", encoding='utf-8') as f:\n",
        "        for key, value in intros.items():\n",
        "            f.write('%s, [%s\\n' % (key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BRyHpFCsTPAw"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # cont_file = path+\"output/resume_content.txt\"\n",
        "    # expr_file = path+\"output/experiences.txt\"\n",
        "    # skil_file = path+\"output/top_skill.txt\"\n",
        "\n",
        "    cont_file = \"output/resume_content.txt\"\n",
        "    expr_file = \"output/experiences.txt\"\n",
        "    skil_file = \"output/top_skill.txt\"\n",
        "    \n",
        "    edu_info = extract_edu(cont_file)\n",
        "    expr_info = extract_expr(expr_file)\n",
        "    skil_info = extract_skill(skil_file)\n",
        "    \n",
        "    intros = generate_intro(edu_info, expr_info, skil_info)\n",
        "    # out_intro_file = path+\"output/intros.txt\"\n",
        "    out_intro_file = \"output/intros.txt\"\n",
        "    \n",
        "    save_intro(out_intro_file, intros)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}